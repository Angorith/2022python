{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "0823 73일차ML(pygame, gym[toy_text], 계단모형, 상태전이 확률분포와 정책, 최적 정책,가치함수, 벨만 기대방정식).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPb15Eos9wfg6y7LEmQ6GHz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/myungjinma/Python_Study/blob/main/0823_73%EC%9D%BC%EC%B0%A8ML(pygame%2C_gym%5Btoy_text%5D%2C_%EA%B3%84%EB%8B%A8%EB%AA%A8%ED%98%95%2C_%EC%83%81%ED%83%9C%EC%A0%84%EC%9D%B4_%ED%99%95%EB%A5%A0%EB%B6%84%ED%8F%AC%EC%99%80_%EC%A0%95%EC%B1%85%2C_%EC%B5%9C%EC%A0%81_%EC%A0%95%EC%B1%85%2C%EA%B0%80%EC%B9%98%ED%95%A8%EC%88%98%2C_%EB%B2%A8%EB%A7%8C_%EA%B8%B0%EB%8C%80%EB%B0%A9%EC%A0%95%EC%8B%9D).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9giGxFL3eZ_f",
        "outputId": "636f616b-fc24-47e9-d5ad-b77be3757a53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.7/dist-packages (2.1.2)\n"
          ]
        }
      ],
      "source": [
        "pip install pygame"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gym[toy_text]"
      ],
      "metadata": {
        "id": "ezAqzZDBeij7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f528799-d3e8-4699-dae5-c8d998817ca7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[toy_text] in /usr/local/lib/python3.7/dist-packages (0.25.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[toy_text]) (4.12.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[toy_text]) (0.0.8)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[toy_text]) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[toy_text]) (1.21.6)\n",
            "Collecting pygame==2.1.0\n",
            "  Downloading pygame-2.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 18.3 MB 83 kB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[toy_text]) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[toy_text]) (3.8.1)\n",
            "Installing collected packages: pygame\n",
            "  Attempting uninstall: pygame\n",
            "    Found existing installation: pygame 2.1.2\n",
            "    Uninstalling pygame-2.1.2:\n",
            "      Successfully uninstalled pygame-2.1.2\n",
            "Successfully installed pygame-2.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\""
      ],
      "metadata": {
        "id": "wvHT9gSFekqX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym"
      ],
      "metadata": {
        "id": "bxlz5OcQel0E"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"FrozenLake-v1\", is_slippery=False)"
      ],
      "metadata": {
        "id": "7-MysDLXenTX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02fbf03e-e9ed-4077-b0db-01caf117a511"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:330: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.observation_space"
      ],
      "metadata": {
        "id": "SipdR44GeoZr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6970d3ff-7515-479d-f849-d74fba627395"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discrete(16)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.action_space"
      ],
      "metadata": {
        "id": "ucOeEoBoepkV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ff408ed-70b9-4c55-aa16-a24fbff074b0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discrete(4)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_trial=20"
      ],
      "metadata": {
        "id": "3nkEC509eque"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.reset()\n",
        "episode=[]"
      ],
      "metadata": {
        "id": "HM48k7GKertV"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(n_trial):\n",
        "    action = env.action_space.sample()\n",
        "    obs, reward, done, info = env.step(action)\n",
        "    episode.append([action, reward, obs])\n",
        "    env.render()\n",
        "    if done:\n",
        "        break\n",
        "print('-------[행동, 보상, 상태]--------')\n",
        "print(episode)\n",
        "env.close()"
      ],
      "metadata": {
        "id": "Y9jp45hHes6G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64550742-242b-4c80-a7b5-e28e788f1f9a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:58: DeprecationWarning: \u001b[33mWARN: You are calling render method, but you didn't specified the argument render_mode at environment initialization. To maintain backward compatibility, the environment will render in human mode.\n",
            "If you want to render in human mode, initialize the environment in this way: gym.make('EnvName', render_mode='human') and don't call the render method.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"You are calling render method, \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------[행동, 보상, 상태]--------\n",
            "[[0, 0.0, 0], [1, 0.0, 4], [0, 0.0, 4], [2, 0.0, 5]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#계산 모형\n",
        "\n",
        "마르코프 결정 과정 : 행동, 상태변화, 보상을 정의 하고 정책(최적 정책)을 찾는 일을 지원하는 수학적인 틀을 얘기한다.\n",
        "\n",
        "마르코프 결정 과정의 핵심적인 문제는 의사결정자의 의사결정 정책(policy)\n",
        "\n",
        "$$\n",
        "π\n",
        "$$\n",
        "를 구하는 것이다."
      ],
      "metadata": {
        "id": "JEGPjJPgevss"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#상태 전이 확률 분포와 정책\n",
        "\n",
        "최적 정책을 알아내자\n",
        "\n"
      ],
      "metadata": {
        "id": "UOtBYLIxe-nl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#9.2.2 최적 정책\n",
        "\n",
        "정책은 현재 상태에서 어떤 행동을 취할지를 결정하는 확률 분포이다.\n",
        "\n",
        "학습 알고리즘이 해야 할 일 : 누적 보상을 최대화하는 \"최적 정책\"을 알아내야 함\n",
        "\n",
        "즉, P(a|s) 는 상태 s에서 행동 a가 일어날 확률이다. 그러므로 정책은 확률 분포로 표현 된다. 이 말을 이해하기 위해 FrozenLake의 예제를 보자\n",
        "\n",
        "그렇다면 누적 보상이 최대가 되는 각 상태에서의 확률을 계산하기위해서는 어떻게 해야 하는 가? => 가치 함수를 이용한다."
      ],
      "metadata": {
        "id": "6EHLojs1fxeM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#가치 함수\n",
        "\n",
        ": 상태 s에서 정책이 얼마나 많은 보상을 제공할지를 계산하는 함수\n",
        "\n",
        "$$\n",
        "v_π(s),\\quad \\forall s ∈ \\mathcal{S} \\quad \\quad \\quad (9.4)\n",
        "$$\n",
        "\n",
        "* 최적 정책\n",
        ": 가치함수를 최대화 하는 정책\n",
        "\n",
        "$$π = aragmaru_π(s),\\quad \\forall s ∈ \\mathcal{S} \\quad \\quad \\quad (9.5)\n",
        "$$\n",
        "\n",
        "그러나 가치 함수를 어떻게 계산하는지는 제공하고 있지 않다.\n"
      ],
      "metadata": {
        "id": "TZI4klWGf4UJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#가치함수의 계산\n",
        "\n",
        "$$\n",
        "v_π(s) = E(r|s) = \\sum_{s에서 출발하는 모든 에피소드 z} P(z)r(z), \\quad \\forall s ∈ \\mathcal{S} \\quad \\quad \\quad (9.6)\n",
        "$$\n",
        "\n",
        "가치 함수는 상태 s에서 발생할 수 있는 누적 보상액 r의 기댓값이다.\n",
        "\n",
        "r의 기댓값은 s에서 출발하는 모든 에피소드에 대해 발생확률 p(z)와 누적 보상액 r(z)의 곱의 합이다.\n",
        "\n",
        "가치 함수를 계산 하는 방법 (Pi_2 기준 <- Pi_1 정책에서는 0.5 였으므로 Pi_2가 Pi_1보다 최적화 되어 있다.)\n",
        "\n",
        "\n",
        "$$\n",
        "v_π(10) = E(z_2)r(z_2) = P(1|10)P(2|14)r(z_2) = 1*1*1 = 1\n",
        "$$"
      ],
      "metadata": {
        "id": "AjdlnPNIgAh9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#벨만 기대방정식\n",
        "\n",
        "$$v{\\pi} (s) = \\sum{a \\in \\mathcal{A} (s)}P(a|s)(r+v_{\\pi}(s')), \\quad \\forall s ∈ \\mathcal{S} \\quad \\quad \\quad (9.7)\n",
        "$$"
      ],
      "metadata": {
        "id": "Oz3pJ-EngEQQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#할인율을 적용한 누적 보상액\n",
        "\n",
        "gamma = 0 이면 현재 보상액만 고려한게 되어 근시안적인 추정\n",
        "\n",
        "gamma=1 이면 미래에 발생하는 모든 보상을 같은 가중치로 계산\n",
        "\n",
        "에피소드가 무한일 경우에 할인율이 없으면 누적 보상액이 무한대가 되므로 gamma는 1보다 작아야 한다."
      ],
      "metadata": {
        "id": "5m3iv8ybgHvh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#상태 가치 함수와 벨만기대방정식\n",
        "\n",
        "보상의 할인율 gamma를 적용한 벨만기대방정식은 상태 가치 함수가 된다.\n",
        "\n",
        "$$v{\\pi} (s) = \\sum{a \\in \\mathcal{A} (s)}P(a|s)(r+γv_{\\pi}(s')), \\forall s ∈ \\mathcal{S} \\quad \\quad \\quad (9.9)\n",
        "$$"
      ],
      "metadata": {
        "id": "m5bqcp3kgLzR"
      }
    }
  ]
}